{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43490178-1646-4cba-9119-7b7e724a68c4",
   "metadata": {},
   "source": [
    "# Reading File Directories and Exploring WordNet\n",
    "\n",
    "This notebook provides some guidance on working with file directories for a text corpus. It's also introducing some things we can explore with the WordNet dictionary accessible through NLTK. \n",
    "\n",
    "Read about [Wordnet in Chapter 2, section 5.2](https://www.nltk.org/book/ch02.html#wordnet) of the NLTK book. \n",
    "Credits: Some of our explanations are distilled from [David Birnbaum's introductory Wordnet notebook](https://github.com/djbpitt/wordnet/blob/master/Wordnet.ipynb) that we used to explore ambiguity of spooky words in projects some years ago! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4388f1b3-a67c-478d-8acb-dd75d518e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e173c67c-f20d-4f9a-b92a-2e4398a767b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('clear.n.01'),\n",
       " Synset('open.n.01'),\n",
       " Synset('unclutter.v.01'),\n",
       " Synset('clear.v.02'),\n",
       " Synset('clear_up.v.04'),\n",
       " Synset('authorize.v.01'),\n",
       " Synset('clear.v.05'),\n",
       " Synset('pass.v.09'),\n",
       " Synset('clear.v.07'),\n",
       " Synset('clear.v.08'),\n",
       " Synset('clear.v.09'),\n",
       " Synset('clear.v.10'),\n",
       " Synset('clear.v.11'),\n",
       " Synset('clear.v.12'),\n",
       " Synset('net.v.02'),\n",
       " Synset('net.v.01'),\n",
       " Synset('gain.v.08'),\n",
       " Synset('clear.v.16'),\n",
       " Synset('clear.v.17'),\n",
       " Synset('acquit.v.01'),\n",
       " Synset('clear.v.19'),\n",
       " Synset('clear.v.20'),\n",
       " Synset('clear.v.21'),\n",
       " Synset('clear.v.22'),\n",
       " Synset('clear.v.23'),\n",
       " Synset('clear.v.24'),\n",
       " Synset('clear.a.01'),\n",
       " Synset('clear.s.02'),\n",
       " Synset('clear.s.03'),\n",
       " Synset('clear.a.04'),\n",
       " Synset('clear.s.05'),\n",
       " Synset('clear.s.06'),\n",
       " Synset('clean.s.03'),\n",
       " Synset('clear.s.08'),\n",
       " Synset('clear.s.09'),\n",
       " Synset('well-defined.a.02'),\n",
       " Synset('clear.a.11'),\n",
       " Synset('clean.s.02'),\n",
       " Synset('clear.s.13'),\n",
       " Synset('clear.s.14'),\n",
       " Synset('clear.s.15'),\n",
       " Synset('absolved.s.01'),\n",
       " Synset('clear.s.17'),\n",
       " Synset('clear.r.01'),\n",
       " Synset('clearly.r.04')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOKE TEST: Explore Wordnet for specific words.\n",
    "wn.synsets('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61622953-4ec6-44fc-abda-061246343662",
   "metadata": {},
   "source": [
    "### Look at some synset data from WordNet\n",
    "Choose one of the synsets by its identifier, and lets explore what you can see with it.\n",
    "In the next couple of cells we explore how WordNet shares information: \n",
    "\n",
    "Wordnet shows:\n",
    "* A representative word that stands for set of meanings (a \"synset\"). There may be other representative words, and that same representative word might be used in different senses (and have multiple synsets).  \n",
    "* A part of speech (POS) identifier, like “n” for ‘noun’ or “v” for ‘verb’.\n",
    "* A two-digit number that distinguishes different synsets that may have the same head word and the same POS, but that convey different meaning. For example, the synsets 'ghost.n.01' and 'ghost.n.02' are two different nominal meanings that can be expressed by the lexeme “ghost.”\n",
    "\n",
    "In WordNet you can request:\n",
    "* the available **lemmas** which mean \"lexemes\" or the available set of words associated with a particular synset.\n",
    "* the **definition** associated with a synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493873cf-a9fb-42b2-8184-8eb5bbbabe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('acquit.v.01.acquit'),\n",
       " Lemma('acquit.v.01.assoil'),\n",
       " Lemma('acquit.v.01.clear'),\n",
       " Lemma('acquit.v.01.discharge'),\n",
       " Lemma('acquit.v.01.exonerate'),\n",
       " Lemma('acquit.v.01.exculpate')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('acquit.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b39aed6-7598-415d-b8f5-ede96f6bb3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquit\n",
      "assoil\n",
      "clear\n",
      "discharge\n",
      "exonerate\n",
      "exculpate\n"
     ]
    }
   ],
   "source": [
    "# For every lem in one of the synsets, isolate just the names without the extra stuff:\n",
    "for lem in wn.synset('acquit.v.01').lemmas():\n",
    "    print(lem.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b947f920-158f-4faf-9e8a-129cdac58235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('clear.n.01') :  ['clear'] :  1\n",
      "Synset('open.n.01') :  ['open', 'clear'] :  2\n",
      "Synset('unclutter.v.01') :  ['unclutter', 'clear'] :  2\n",
      "Synset('clear.v.02') :  ['clear'] :  1\n",
      "Synset('clear_up.v.04') :  ['clear_up', 'clear', 'light_up', 'brighten'] :  4\n",
      "Synset('authorize.v.01') :  ['authorize', 'authorise', 'pass', 'clear'] :  4\n",
      "Synset('clear.v.05') :  ['clear'] :  1\n",
      "Synset('pass.v.09') :  ['pass', 'clear'] :  2\n",
      "Synset('clear.v.07') :  ['clear'] :  1\n",
      "Synset('clear.v.08') :  ['clear'] :  1\n",
      "Synset('clear.v.09') :  ['clear', 'top'] :  2\n",
      "Synset('clear.v.10') :  ['clear', 'clear_up', 'shed_light_on', 'crystallize', 'crystallise', 'crystalize', 'crystalise', 'straighten_out', 'sort_out', 'enlighten', 'illuminate', 'elucidate'] :  12\n",
      "Synset('clear.v.11') :  ['clear'] :  1\n",
      "Synset('clear.v.12') :  ['clear'] :  1\n",
      "Synset('net.v.02') :  ['net', 'clear'] :  2\n",
      "Synset('net.v.01') :  ['net', 'sack', 'sack_up', 'clear'] :  4\n",
      "Synset('gain.v.08') :  ['gain', 'take_in', 'clear', 'make', 'earn', 'realize', 'realise', 'pull_in', 'bring_in'] :  9\n",
      "Synset('clear.v.16') :  ['clear'] :  1\n",
      "Synset('clear.v.17') :  ['clear'] :  1\n",
      "Synset('acquit.v.01') :  ['acquit', 'assoil', 'clear', 'discharge', 'exonerate', 'exculpate'] :  6\n",
      "Synset('clear.v.19') :  ['clear', 'solve'] :  2\n",
      "Synset('clear.v.20') :  ['clear'] :  1\n",
      "Synset('clear.v.21') :  ['clear'] :  1\n",
      "Synset('clear.v.22') :  ['clear'] :  1\n",
      "Synset('clear.v.23') :  ['clear'] :  1\n",
      "Synset('clear.v.24') :  ['clear', 'clear_up'] :  2\n",
      "Synset('clear.a.01') :  ['clear'] :  1\n",
      "Synset('clear.s.02') :  ['clear'] :  1\n",
      "Synset('clear.s.03') :  ['clear', 'open'] :  2\n",
      "Synset('clear.a.04') :  ['clear'] :  1\n",
      "Synset('clear.s.05') :  ['clear'] :  1\n",
      "Synset('clear.s.06') :  ['clear'] :  1\n",
      "Synset('clean.s.03') :  ['clean', 'clear', 'light', 'unclouded'] :  4\n",
      "Synset('clear.s.08') :  ['clear', 'unmortgaged'] :  2\n",
      "Synset('clear.s.09') :  ['clear', 'clean-cut', 'clear-cut'] :  3\n",
      "Synset('well-defined.a.02') :  ['well-defined', 'clear'] :  2\n",
      "Synset('clear.a.11') :  ['clear'] :  1\n",
      "Synset('clean.s.02') :  ['clean', 'clear'] :  2\n",
      "Synset('clear.s.13') :  ['clear'] :  1\n",
      "Synset('clear.s.14') :  ['clear'] :  1\n",
      "Synset('clear.s.15') :  ['clear', 'decipherable', 'readable'] :  3\n",
      "Synset('absolved.s.01') :  ['absolved', 'clear', 'cleared', 'exculpated', 'exonerated', 'vindicated'] :  6\n",
      "Synset('clear.s.17') :  ['clear', 'percipient'] :  2\n",
      "Synset('clear.r.01') :  ['clear', 'all_the_way'] :  2\n",
      "Synset('clearly.r.04') :  ['clearly', 'clear'] :  2\n"
     ]
    }
   ],
   "source": [
    "# Here we'll just return the count of the available lemmas for each synset for \"clear\"\n",
    "# In this output we're just surveying the data as lists:\n",
    "for synset in wn.synsets('clear'):\n",
    "    print(synset, \": \", synset.lemma_names(), \": \", len(synset.lemma_names()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159bfaa4-e161-42c3-bee0-286030890e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clear'] : Part of Speech:  n : Definition:  the state of being free of suspicion\n",
      "['open', 'clear'] : Part of Speech:  n : Definition:  a clear or unobstructed space or expanse of land or water\n",
      "['unclutter', 'clear'] : Part of Speech:  v : Definition:  rid of obstructions\n",
      "['clear'] : Part of Speech:  v : Definition:  make a way or path by removing objects\n",
      "['clear_up', 'clear', 'light_up', 'brighten'] : Part of Speech:  v : Definition:  become clear\n",
      "['authorize', 'authorise', 'pass', 'clear'] : Part of Speech:  v : Definition:  grant authorization or clearance for\n",
      "['clear'] : Part of Speech:  v : Definition:  remove\n",
      "['pass', 'clear'] : Part of Speech:  v : Definition:  go unchallenged; be approved\n",
      "['clear'] : Part of Speech:  v : Definition:  be debited and credited to the proper bank accounts\n",
      "['clear'] : Part of Speech:  v : Definition:  go away or disappear\n",
      "['clear', 'top'] : Part of Speech:  v : Definition:  pass by, over, or under without making contact\n",
      "['clear', 'clear_up', 'shed_light_on', 'crystallize', 'crystallise', 'crystalize', 'crystalise', 'straighten_out', 'sort_out', 'enlighten', 'illuminate', 'elucidate'] : Part of Speech:  v : Definition:  make free from confusion or ambiguity; make clear\n",
      "['clear'] : Part of Speech:  v : Definition:  free from payment of customs duties, as of a shipment\n",
      "['clear'] : Part of Speech:  v : Definition:  clear from impurities, blemishes, pollution, etc.\n",
      "['net', 'clear'] : Part of Speech:  v : Definition:  yield as a net profit\n",
      "['net', 'sack', 'sack_up', 'clear'] : Part of Speech:  v : Definition:  make as a net profit\n",
      "['gain', 'take_in', 'clear', 'make', 'earn', 'realize', 'realise', 'pull_in', 'bring_in'] : Part of Speech:  v : Definition:  earn on some commercial or business transaction; earn as salary or wages\n",
      "['clear'] : Part of Speech:  v : Definition:  sell\n",
      "['clear'] : Part of Speech:  v : Definition:  pass an inspection or receive authorization\n",
      "['acquit', 'assoil', 'clear', 'discharge', 'exonerate', 'exculpate'] : Part of Speech:  v : Definition:  pronounce not guilty of criminal charges\n",
      "['clear', 'solve'] : Part of Speech:  v : Definition:  settle, as of a debt\n",
      "['clear'] : Part of Speech:  v : Definition:  make clear, bright, light, or translucent\n",
      "['clear'] : Part of Speech:  v : Definition:  rid of instructions or data\n",
      "['clear'] : Part of Speech:  v : Definition:  remove (people) from a building\n",
      "['clear'] : Part of Speech:  v : Definition:  remove the occupants of\n",
      "['clear', 'clear_up'] : Part of Speech:  v : Definition:  free (the throat) by making a rasping sound\n",
      "['clear'] : Part of Speech:  a : Definition:  readily apparent to the mind\n",
      "['clear'] : Part of Speech:  s : Definition:  free from confusion or doubt\n",
      "['clear', 'open'] : Part of Speech:  s : Definition:  affording free passage or view\n",
      "['clear'] : Part of Speech:  a : Definition:  allowing light to pass through\n",
      "['clear'] : Part of Speech:  s : Definition:  free from contact or proximity or connection\n",
      "['clear'] : Part of Speech:  s : Definition:  characterized by freedom from troubling thoughts (especially guilt)\n",
      "['clean', 'clear', 'light', 'unclouded'] : Part of Speech:  s : Definition:  (of sound or color) free from anything that dulls or dims\n",
      "['clear', 'unmortgaged'] : Part of Speech:  s : Definition:  (especially of a title) free from any encumbrance or limitation that presents a question of fact or law\n",
      "['clear', 'clean-cut', 'clear-cut'] : Part of Speech:  s : Definition:  clear and distinct to the senses; easily perceptible\n",
      "['well-defined', 'clear'] : Part of Speech:  a : Definition:  accurately stated or described\n",
      "['clear'] : Part of Speech:  a : Definition:  free from clouds or mist or haze\n",
      "['clean', 'clear'] : Part of Speech:  s : Definition:  free of restrictions or qualifications\n",
      "['clear'] : Part of Speech:  s : Definition:  free from flaw or blemish or impurity\n",
      "['clear'] : Part of Speech:  s : Definition:  clear of charges or deductions\n",
      "['clear', 'decipherable', 'readable'] : Part of Speech:  s : Definition:  easily deciphered\n",
      "['absolved', 'clear', 'cleared', 'exculpated', 'exonerated', 'vindicated'] : Part of Speech:  s : Definition:  freed from any question of guilt\n",
      "['clear', 'percipient'] : Part of Speech:  s : Definition:  characterized by ease and quickness in perceiving\n",
      "['clear', 'all_the_way'] : Part of Speech:  r : Definition:  completely\n",
      "['clearly', 'clear'] : Part of Speech:  r : Definition:  in an easily perceptible manner\n"
     ]
    }
   ],
   "source": [
    "# Ask for the definitions and part of speech available for a word lemma names in WordNet\n",
    "for synset in wn.synsets('clear'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750f700-bf75-4cb6-892e-453df3d3deab",
   "metadata": {},
   "source": [
    "## Ambiguity from WordNet's point of view\n",
    "When words are ambiguous, they can multiple shades of meaning or different ways we could interpret them. How we decide on the meaning depends on the context of the words. WordNet can't tell you everything about how you could interpret every word in your project, but when words are available in its vast database, you can explore how many synsets (or possible meaningful interpretations) it has on file for that word. You can find which words depend the most on specific contexts for their meaning. \n",
    "\n",
    "To find out WordNet's sense of how ambiguous a word is, you could just count the number of available synsets for it with Python's **len()**.\n",
    "Note: when you want ALL the synsets, use **wn.synsets** (plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0afc7c-31ac-462a-b88c-8ebb841b01d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word  creep belongs to  8 synsets in WordNet.\n",
      "The word  eldritch belongs to  1 synsets in WordNet.\n",
      "The word  horror belongs to  3 synsets in WordNet.\n",
      "The word  cry belongs to  12 synsets in WordNet.\n",
      "The word  scream belongs to  6 synsets in WordNet.\n",
      "The word  ghost belongs to  7 synsets in WordNet.\n",
      "The word  scare belongs to  4 synsets in WordNet.\n"
     ]
    }
   ],
   "source": [
    "spooky_words = ['creep', 'eldritch', 'horror', 'cry', 'scream', 'ghost', 'scare']\n",
    "for w in spooky_words:\n",
    "    synsets = len(wn.synsets(w))\n",
    "    print(\"The word \", w, \"belongs to \", synsets, \"synsets in WordNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd59ab9-4763-4ca0-9ae3-bb5d38fe994f",
   "metadata": {},
   "source": [
    "### WordNet's morphy: for inflected forms like plural vs. singular\n",
    "Wordnet won't have an entry for plural forms of words (for example). For other things like -ing words, those tend to have their own WordNet entry. \n",
    "\n",
    "When working with project data, **run your word tokens through Wordnet's morphy in case they need to be looked up in a lemmatized form**\n",
    "`wn.morphy('ghosts')`. Try it out on various word forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10215ca7-f49c-4160-bd81-393c6890c44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy('cries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbb99d-6677-4c2b-a77b-b0520c5437f5",
   "metadata": {},
   "source": [
    "### A Workflow for projects exploring ambiguity\n",
    "* Start by isolating a set of words of interest from your project. (One idea: Use nltk's pos tagger to identify all the verbs in your files and retrieve a set of them).\n",
    "* Deliver them to WordNet to retrieve their synset counts\n",
    "* Output the information on each distinct word from Python to a simple XML, TSV, or JSON (We'll probably use XML)\n",
    "* Options:\n",
    "    * Use the frequency distribution plotting in Python to see how frequently ambiguous words are used in your corpus (plotting is a bit limited)\n",
    "    * Write XSLT to map that information back into your XML files: We can use this to **instantiate** the uses of ambiguous words: retrieve counts of how frequently they appear in the text, and plot with SVGs of your own design. (You could make an SVG representing info about the most ambiguous words. You could also make specific SVGs for ambiguous words of interest to show how they're distributed throughout your texts... what would be interesting to visualize?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5b503-aa93-4348-a4f5-b096528a5d61",
   "metadata": {},
   "source": [
    "## Part of Speech (and other related kinds of) Tagging\n",
    "NLTK (and spacY and other language models) offer Part of Speech tagging. We could use this to collect sets of distinct verbs, nouns, adjectives. \n",
    "* We could learn more about the words in a set by sending them to WordNet.\n",
    "* We could also look at words as a network: How often do specific characters rely on a word of interest? Which characters share a word of interest? Or which words of interest are shared by say, male vs. female characters?  (Here's [a student project](http://bamfs.obdurodon.org/allFilms_Results.xhtml) that explored a collection of colorful swear words used by characters in Quentin Tarantino's films.)\n",
    "\n",
    "In our Text Analysis class, you didn't spend time marking specific words, but you could do that using regex search and replace over your texts. But you could also get some help from POS tagging from NLTK, and mapping that information back to your XML files for further analysis. So let's explore that.  We'll look at POS (part of speech) tagging, but note that it's related to Named Entity Recognition and sentiment analysis. \n",
    "\n",
    "(Full disclosure: We like POS for reasons of interesting patterns it can display without overdetermining what the data means in context. You can show interesting patterns, especially by working with POS tagging together with WordNet analyses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b699d-a22f-42bb-9eee-dea1add1f4e8",
   "metadata": {},
   "source": [
    "## How to get NLTK to do POS tagging\n",
    "* Let's open a file, tokenize it, prepare it for nltk to analyze.\n",
    "* Then apply the .pos tagger\n",
    "* What we see in the output is a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58ce8f1a-8b4e-4c21-a53c-e2e23fe8d43b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DIGIT210/python/projectfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[32m      5\u001b[39m filepath = \u001b[33m'\u001b[39m\u001b[33mDIGIT210/python/projectfiles\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.read()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Make a list of tokens in your text. \u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# tokenList = f.split()\u001b[39;00m\n\u001b[32m      9\u001b[39m tokenList = word_tokenize(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'DIGIT210/python/projectfiles'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "filepath = 'DIGIT210/python/projectfiles'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# Make a list of tokens in your text. \n",
    "# tokenList = f.split()\n",
    "tokenList = word_tokenize(f)\n",
    "# How is NLTK's word_tokenize() different from just splitting on spaces? Here's an example of how it's different: \n",
    "# Look for \"don't\" in the output of this cell and see how it's split.\n",
    "print(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82c80516-cd28-4b7f-adc6-2572a39726a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Reduce the complexity by: 1) lowercasing them, and 2) returning the set() of words (remove multiple of the same value)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m lowercaseTokens = [token.lower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenList\u001b[49m]\n\u001b[32m      3\u001b[39m uniqueTokens = \u001b[38;5;28mset\u001b[39m(lowercaseTokens)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(uniqueTokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenList' is not defined"
     ]
    }
   ],
   "source": [
    "# Reduce the complexity by: 1) lowercasing them, and 2) returning the set() of words (remove multiple of the same value)\n",
    "lowercaseTokens = [token.lower() for token in tokenList]\n",
    "uniqueTokens = set(lowercaseTokens)\n",
    "print(uniqueTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116d6fd3-e2c8-4dd8-a1d0-2a5f397c68c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniqueTokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Let's try out the NLTK POS tagger on the uniqueTokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pos_tag(\u001b[43muniqueTokens\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'uniqueTokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's try out the NLTK POS tagger on the uniqueTokens\n",
    "pos_tag(uniqueTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93758d5-09c2-46e3-b075-f6bdde741c0c",
   "metadata": {},
   "source": [
    "### Wait...what is that POS category?\n",
    "Find out what a category is by asking NLTK:\n",
    "`nltk.help.upenn_tagset('VBZ')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360fa3b9-2c2e-4515-833c-10cb26345da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtagsets_json\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets_json')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets_json/PY3_json/upenn_tagset.json\u001b[0m\n\n  Searched in:\n    - '/Users/alexanderfisher/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/share/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhelp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupenn_tagset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNNS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/help.py:28\u001b[39m, in \u001b[36mupenn_tagset\u001b[39m\u001b[34m(tagpattern)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupenn_tagset\u001b[39m(tagpattern=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43m_format_tagset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupenn_tagset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagpattern\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/help.py:48\u001b[39m, in \u001b[36m_format_tagset\u001b[39m\u001b[34m(tagset, tagpattern)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_tagset\u001b[39m(tagset, tagpattern=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Load tagset from json file.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     tag_json_file = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhelp/tagsets_json/PY3_json/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtagset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tag_json_file) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m     50\u001b[39m         tagdict = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mtagsets_json\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets_json')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets_json/PY3_json/upenn_tagset.json\u001b[0m\n\n  Searched in:\n    - '/Users/alexanderfisher/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/share/nltk_data'\n    - '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405aeaaa-c8d6-46d8-b7e7-208e7c23968a",
   "metadata": {},
   "source": [
    "We can limit our list of words of interest by asking for words of a particular kind, isolating them by POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b232cb7a-8793-4034-a6b0-d194e5a7d44e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniqueTokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m POStagged = pos_tag(\u001b[43muniqueTokens\u001b[49m)\n\u001b[32m      2\u001b[39m tagsIwant = [\u001b[33m'\u001b[39m\u001b[33mVB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVBZ\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This is a Python list comprehension that'll help us with our list of tuples [(word, tag), (word, tag), ...]\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'uniqueTokens' is not defined"
     ]
    }
   ],
   "source": [
    "POStagged = pos_tag(uniqueTokens)\n",
    "tagsIwant = ['VB', 'VBZ']\n",
    "# This is a Python list comprehension that'll help us with our list of tuples [(word, tag), (word, tag), ...]\n",
    "shortList = [word for word, tag in POStagged if tag in tagsIwant]\n",
    "print(len(shortList))\n",
    "print(shortList)\n",
    "\n",
    "# Fancy string formatting (not super useful here, just reviewing it): \n",
    "# f'My short list is {shortList} and it is this long: {len(shortList)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f37dba-dc58-487c-ad4b-eb169f1e9155",
   "metadata": {},
   "source": [
    "## Send them to WordNet for synset lookup\n",
    "\n",
    "* Use list comprehension\n",
    "* Use wn.morphy to simplify the word to its synset lemma form\n",
    "* While we're at it, let's sort the shortList using `sorted()`.\n",
    "* We noticed that WordNet has its own parts of speech, and they're a lot simpler than NLTK's.\n",
    "    * WordNet's parts of speech are ADJ, ADJ_SAT, ADV, NOUN, VERB:  'a', 's', 'r', 'n', 'v'\n",
    "    * We also noticed that NLTK's part of speech tagging is rather shockingly unreliable on our texts.\n",
    "    * We wonder if spaCy might be better, or if we really need to train the models on our texts by tagging them ourselves.\n",
    "    * You might want to just make a list of all the words that have the same ending, like our [-ing smoke test](https://github.com/newtfire/textAnalysis-Hub/blob/main/Class-Examples/Python/orient-ing.py) from the NLTK book intro in our first Python assignment. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a55e05-77a5-45c9-8add-f2094314c2ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shortList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mshortList\u001b[49m):\n\u001b[32m      2\u001b[39m     lemma = wn.morphy(w)\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'shortList' is not defined"
     ]
    }
   ],
   "source": [
    "for w in sorted(shortList):\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    pos = {synset.pos() for synset in synsets}\n",
    "    if synsets:\n",
    "       \n",
    "        \n",
    "        print(f\" Word: {w}, POS-according-to-WordNet {pos} Number of Synsets (Ambiguity): {len(synsets)}\") \n",
    "              \n",
    "        \n",
    "        # for syn in synsets:\n",
    "        #   print(f\"  Synset: {syn.name()}, Definition: {syn.definition()}}\")\n",
    "    \n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd9132-801c-415e-bf59-ed9712a02911",
   "metadata": {},
   "source": [
    "## Read in just one file from a directory in your repo\n",
    "Open one of your text files in your repo for reading.\n",
    "In this example, we'll climb directories, and that means we'll use the os library to show you how to handle filepaths.\n",
    "\n",
    "When your file isn't immediately in the same folder as your Python script, and you need to climb for it, start with os library by getting the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e348fd7-3de0-40aa-a76f-688d195775a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexanderfisher/Git/fish-fry/DIGIT210/python\n"
     ]
    }
   ],
   "source": [
    "# cwd is my shorthand for \"current working directory\"\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cfb7420-b1d2-487d-9f5e-372a218afc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../grimm.txt\n"
     ]
    }
   ],
   "source": [
    "# climb up one directory and retrieve a file. (here's how you would do that)\n",
    "# (ADAPT THIS CODE TO REACH DOWN OR UP AS NEEDED.)\n",
    "filepath = '../grimm.txt'\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc90f2c-7478-49d2-953d-f94c3b994f00",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../grimm.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Now, Python must OPEN and READ the text file in order to process it:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.read()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# readFile = f.read()\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../grimm.txt'"
     ]
    }
   ],
   "source": [
    "# Now, Python must OPEN and READ the text file in order to process it:\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# readFile = f.read()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432701a-b8d5-4ff0-9be6-3f3002461a14",
   "metadata": {},
   "source": [
    "## Read in some project data from a collection of text files\n",
    "You have a file directory with some text files probably near your Python script. \n",
    "\n",
    " From the \"for loop\" in the next cell, we can then write code to process information about each file separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76d51112-96be-4409-884c-aa77853b81fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['webScrapers',\n",
       " '.DS_Store',\n",
       " 'WordNet-and-Files.ipynb',\n",
       " 'Class Examples',\n",
       " 'projectfiles',\n",
       " '.venv',\n",
       " 'nlp1.py',\n",
       " '.ipynb_checkpoints',\n",
       " '.idea']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember, we defined cwd as our current working directory holding this file.\n",
    "# list directories:\n",
    "os.listdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf29225-9f02-4b60-b4ee-231302ba4872",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m coll = os.path.join(cwd, \u001b[33m'\u001b[39m\u001b[33mhughes-txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'"
     ]
    }
   ],
   "source": [
    "coll = os.path.join(cwd, 'hughes-txt')\n",
    "os.listdir(coll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe0019-0692-4aed-84f9-ec4ca6f3a69b",
   "metadata": {},
   "source": [
    "## Processing the directory as one corpus\n",
    "What if we want to create an NLTK corpus of these texts and process them as one unit? See the [NLTK book chapter 2 section 1.9](https://www.nltk.org/book/ch02.html#loading-your-own-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9866da8-3164-4c55-b6cd-c8028ce33dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      2\u001b[39m    \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      3\u001b[39m         filepath = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(coll):\n",
    "   if file.endswith(\".txt\"):\n",
    "        filepath = f\"{coll}/{file}\"\n",
    "        print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a78248ca-250a-4253-9964-209e01f7d40e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PlaintextCorpusReader\n\u001b[32m      2\u001b[39m corpus_root = \u001b[33m'\u001b[39m\u001b[33mhughes-txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m corpus = \u001b[43mPlaintextCorpusReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m corpus.fileids()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check on one file in the collection\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# corpus.words('breakfast.txt')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/corpus/reader/plaintext.py:62\u001b[39m, in \u001b[36mPlaintextCorpusReader.__init__\u001b[39m\u001b[34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     39\u001b[39m     root,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     encoding=\u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m ):\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33;03m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43mCorpusReader\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m._word_tokenizer = word_tokenizer\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m._sent_tokenizer = sent_tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/corpus/reader/api.py:80\u001b[39m, in \u001b[36mCorpusReader.__init__\u001b[39m\u001b[34m(self, root, fileids, encoding, tagset)\u001b[39m\n\u001b[32m     78\u001b[39m         root = ZipFilePathPointer(zipfile, zipentry)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         root = \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, PathPointer):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCorpusReader: expected a string or a PathPointer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/fish-fry/DIGIT210/python/.venv/lib/python3.12/site-packages/nltk/data.py:311\u001b[39m, in \u001b[36mFileSystemPathPointer.__init__\u001b[39m\u001b[34m(self, _path)\u001b[39m\n\u001b[32m    309\u001b[39m _path = os.path.abspath(_path)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(_path):\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % _path)\n\u001b[32m    312\u001b[39m \u001b[38;5;28mself\u001b[39m._path = _path\n",
      "\u001b[31mOSError\u001b[39m: No such file or directory: '/Users/alexanderfisher/Git/fish-fry/DIGIT210/python/hughes-txt'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'hughes-txt'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "corpus.fileids()\n",
    "# Check on one file in the collection\n",
    "# corpus.words('breakfast.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00fe8c7-d3ad-4654-8d01-2429bce7a20f",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43830fc0-3079-4648-baab-83cadbc2e6a5",
   "metadata": {},
   "source": [
    "## For Wednesday April 9: YOUR TURN!\n",
    "What do you need to do to process texts in your directory with NLTK tools?\n",
    "You will need to:\n",
    "* Turn your text(s) into a list of tokens!\n",
    "* Then you can process those tokens with NLTK\n",
    "\n",
    "### Your assignment is...\n",
    "Split up your texts into a list of tokens, and do some new NLTK processing of them.\n",
    "Look Stuff Up: See if you can use NLTK to output a **set** of a certain kind of word: could be...\n",
    "* **part of speech** (find out how NLTK can retrieve pos (part-of-speech) information). Retrieve pos information!\n",
    "* Pull a **set** of all the tokens that share a specific part of speech. Make a set() because it removes multiple instances: (it's the same as taking distinct-values() in XPath. (Want all the adjectives? All the verbs? etc.)\n",
    "* Do something interesting with NTLK over that set of words. Try out wordnet synsets, or experiment with frequency distributions, or something else that looks nifty in NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37af2f-710c-4b10-9511-8c4e87ef6980",
   "metadata": {},
   "source": [
    "## For Friday April 11: YOUR TURN!\n",
    "Continue applying what you're learning from this notebook.\n",
    "Take some project files, read and tokenize them, and explore them for Wordnet data.\n",
    "Retrieve info on a limited set of words of your choice (tagged for POS probably, but you can explore!)\n",
    "Output ambiguity data on each word by finding the len() of its synsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a11d8-4f5d-491d-9207-5f14364edd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085a1f0-1df2-4f34-bb6b-20f0bf1343d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048095-eb0e-4c5c-acc9-de8dd86f77cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66958217-ed21-41e6-a50f-ddd1707bc0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d979de-5154-4fc4-8b6b-c779c59af55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2ab7b-bc86-43c8-a086-2c5d436cb94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419141b-beea-4512-867a-814a49026274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
